{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import wordninja\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_train.head()\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet_info(tweets):\n",
    "    tweetTokenizer = TweetTokenizer(strip_handles=True)\n",
    "    \n",
    "    tokenized_tweets = []\n",
    "    tokenized_tweets_no_hashtags = []\n",
    "    tokenized_final_tweets = []\n",
    "    for  tweet in tweets:\n",
    "        \n",
    "        tweet_tokens = tweetTokenizer.tokenize(tweet)\n",
    "        punctuation = ['?','!',',','.',';','(',')',':','\\'','-']\n",
    "        #getting rid of punctuation\n",
    "        tweet_tokens = [token.lower() for token in tweet_tokens if token not in punctuation]#is alpha nu merge\n",
    "\n",
    "        tweet_tokens_no_hashtags = []\n",
    "        tweet_final = []\n",
    "        \n",
    "        for token in tweet_tokens:\n",
    "            \n",
    "            #split the hashtag tokens and remove stopwords\n",
    "            if token[0] == '#':\n",
    "                split_hashtag = wordninja.split(token)\n",
    "                \n",
    "                for hashtag_token in split_hashtag:\n",
    "                    if hashtag_token not in stop_words:\n",
    "                        tweet_tokens_no_hashtags.append(hashtag_token)\n",
    "            else:\n",
    "                if token not in stop_words:\n",
    "                    tweet_tokens_no_hashtags.append(token) \n",
    "                \n",
    "        tokenized_tweets.append(tweet_tokens)\n",
    "        tokenized_tweets_no_hashtags.append(tweet_tokens_no_hashtags)\n",
    "        \n",
    "        \n",
    "        #lemmatize\n",
    "        #final_tweet = [lemma_token.lemma_ for lemma_token in nlp(token) for token in tweet_tokens_no_hashtags ]\n",
    "        final_tweet = [lemmatizer.lemmatize(token) for token in tweet_tokens_no_hashtags]\n",
    "        tokenized_final_tweets.append(final_tweet)\n",
    "        \n",
    "    #print(\"original:\")    \n",
    "    #print(tweets[0])\n",
    "    #print(\"tokens:\")    \n",
    "    #print(tokenized_tweets[0])\n",
    "    #print(\"tokens no hashtags and no stop words:\")    \n",
    "    #print(tokenized_tweets_no_hashtags[0])\n",
    "    #print(\"lemmatize:\")    \n",
    "    #print(tokenized_final_tweets[0])\n",
    "    \n",
    "    return tokenized_final_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Functia get_tweet_info va preucra datele de input astfel:\n",
    "- va folosi tweet_tokenizer pentru a separa datele deoarece acestea sunt tweet-uri, deci poate fi mai uitl decat word_tokenize\n",
    "- va scapa de semnele de punctuatie si va converti toate literele la litere mici\n",
    "- observam ca exista multe hastag-uri in majoritatea tweet-urilor deci va speara hashtag-urile din tweet-uri deoarece \n",
    "acestea pot contine feature-uri importante ce nu vor fi vazute daca sunt pastrate hashtag-urile intregi. Folosim wordninja \n",
    "pentru a separa cuvintele din hashtag-uri\n",
    "- va elimina stop_word-urile\n",
    "- va aduce fiecare cuvant la forma sa din dictionar folosing lemmatizer-ul din nltk (am incercat sa folosesc lemmatizer-ul din\n",
    "spacy pentru ca este mai performant, dar am avut dificultati in folosirea acestuia si am ajuns la cel din nltk)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(filename):\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    tweets = df['tweet'].values\n",
    "    labels = df['label'].values\n",
    "    \n",
    "    shuffle_stratified = StratifiedShuffleSplit(n_splits=1, test_size=0.2)\n",
    "    #shuffle the data\n",
    "    for train_index, test_index in shuffle_stratified.split(tweets, labels):\n",
    "        X_train, X_test = tweets[train_index], tweets[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    X_train = get_tweet_info(X_train)\n",
    "    X_test = get_tweet_info(X_test)\n",
    "\n",
    "    \n",
    "    X_train = [\" \".join(tweet) for tweet in X_train]\n",
    "    X_test = [\" \".join(tweet) for tweet in X_test]\n",
    "\n",
    "   \n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "In functia get_data obtinem datele de antrenare si de testare. Vom folosi StratifiedShuffleSplit deoarece setul de data nu este \n",
    "balansat (avem mai multe date ce nu reprezinta hatred speech). Prelucram datele, iar apoi vom concatena tweet-urile inapoi \n",
    "folosind join pentru a corespunde cu input-ul necesar de la CountVectorizer\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9562020960425466\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      5945\n",
      "           1       0.70      0.67      0.68       448\n",
      "\n",
      "    accuracy                           0.96      6393\n",
      "   macro avg       0.84      0.82      0.83      6393\n",
      "weighted avg       0.96      0.96      0.96      6393\n",
      "\n",
      "happy little people ð    happyhappy people kitten cat black kitten  ¦\n",
      "[[9.99999954e-01 4.61099755e-08]]\n"
     ]
    }
   ],
   "source": [
    "tweet_train, tweet_test, y_train, y_test = get_data('train.csv')\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "\n",
    "count_vectorizer.fit(tweet_train)\n",
    "X_train = count_vectorizer.transform(tweet_train)\n",
    "X_test = count_vectorizer.transform(tweet_test)\n",
    "\n",
    "model = MultinomialNB(alpha=0.01)#initialize the model\n",
    "model.fit(X_train, y_train)#train the model\n",
    "\n",
    "predictions = model.predict(X_test)#predictions for the test data\n",
    "\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))\n",
    "print(tweet_test[0])\n",
    "print(model.predict_proba(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vom folosi CountVectorizer pentru a mapa fiecare token la o pozitie din matricea rezultata. Obtinem matricile sparse pentru train\n",
    "si test. Antrenam modelul Multinomial Naive Bayes si apoi observam ca avem o acuratete de 96% pe datele de test.\n",
    "\n",
    "Observatie: Performanta modelului nu scade daca nu despartim hashtag-urile folosind wordninja\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
